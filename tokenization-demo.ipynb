{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b99fe9",
   "metadata": {},
   "source": [
    "# Tokenization Demonstration\n",
    "\n",
    "To work with UniRel and similar models, we need to understand how their data is tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137a9b2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde21505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import BertTokenizerFast\n",
    "from dataprocess.data_processor import UniRelDataProcessor\n",
    "from dataprocess.dataset import UniRelSpanDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da3c29",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "We load `bert-base-cased` with similar arguments as used in the UniRel code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c323c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    do_basic_tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8adb5d",
   "metadata": {},
   "source": [
    "When we call the tokenizer on a string, it becomes a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d471045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      " 'input_ids': [101,\n",
      "               1109,\n",
      "               3613,\n",
      "               3058,\n",
      "               17594,\n",
      "               15457,\n",
      "               1166,\n",
      "               1103,\n",
      "               16688,\n",
      "               3676,\n",
      "               119,\n",
      "               102,\n",
      "               0,\n",
      "               0,\n",
      "               0,\n",
      "               0],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=16,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "pprint(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3bf56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] The quick brown fox jumps over the lazy dog. [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "decoding = tokenizer.decode(encoding[\"input_ids\"])\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9ae36",
   "metadata": {},
   "source": [
    "The string is surrounded by special `[CLS]` and `[SEP]` tokens, and the remaining space is filled with the `[PAD]` padding token. Comparing against the `input_ids` of the encoding, we can conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945e4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.decode([0]) == \"[PAD]\"\n",
    "assert tokenizer.decode([101]) == \"[CLS]\"\n",
    "assert tokenizer.decode([102]) == \"[SEP]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358cb71",
   "metadata": {},
   "source": [
    "If the text is longer than the allotted tokens, it is cut short but still surrounded by `[CLS]` and `[SEP]` tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211c29b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] The quick brown fox jumps over [SEP]\n"
     ]
    }
   ],
   "source": [
    "truncated = tokenizer.encode(text, max_length=8, truncation=True)\n",
    "print(tokenizer.decode(truncated))\n",
    "assert len(truncated) == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4365a1aa",
   "metadata": {},
   "source": [
    "## UniRel Data\n",
    "\n",
    "The UniRel `Dataset` loads the concatenated tokens of the text and relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d382e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 672.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "more than 100: 0\n",
      "more than 150: 0\n",
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1]),\n",
      " 'head_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]),\n",
      " 'input_ids': tensor([  101,  3559, 15278, 18082,  2249,  9960,  2349, 11185,  2038, 21715,\n",
      "         5541,   132,  1145,  1120,  6523,  1181,  1531,   117,  3883,  7807,\n",
      "         1513,   118,  1113,   118,  6236,   117,   151,   119,   162,   119,\n",
      "          117,  1351,   122,   118, 16892,   119,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0, 26223,  9004,  2380,  2355,  1771, 20769,  1419,  1583,\n",
      "         3207,  2364,  2515, 12179,  1473, 13351,  1234,  1482, 21052, 11242,\n",
      "         2077, 15979,  9545,  4483,  2450,  1929]),\n",
      " 'span_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]),\n",
      " 'tail_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]),\n",
      " 'token_len_batch': tensor(102),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_len = 100\n",
    "data_processor = UniRelDataProcessor(\n",
    "    root=\"data\",\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_name=\"nyt\",\n",
    ")\n",
    "dataset = UniRelSpanDataset(\n",
    "    data_processor.get_train_sample(n_samples=1, token_len=text_len),\n",
    "    data_processor,\n",
    "    tokenizer,\n",
    "    mode='test',\n",
    "    ignore_label=-100,\n",
    "    model_type='bert',\n",
    "    ngram_dict=None,\n",
    "    max_length=text_len+2,\n",
    "    predict=True,\n",
    "    eval_type=\"test\"\n",
    ")\n",
    "example = dataset[0]\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a54bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Massachusetts ASTON MAGNA Great Barrington ; also at Bard College, Annandale - on - Hudson, N. Y., July 1 - Aug. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] advisors founders industry holding founded shareholder company country administrative capital contains neighbor death geographic people children ethnicity nationality lived birthplace profession religion location teams\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(example[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb6044",
   "metadata": {},
   "source": [
    "### Sequence Fields\n",
    "\n",
    "Of the many fields in the dictionary, `attention_mask`, `input_ids`, and `token_type_ids` are 1-dimensional and have the same lengths. They correspond to the text concatenated with relations.\n",
    "\n",
    "The relation tokens (one word representing each relation) are always aligned to the end of the sequence. If the text is shorter than the allotted length, then there are padding tokens added between the text and the relations, as in the sample above. If the text is longer, it is truncated.\n",
    "\n",
    "Notably, the `[CLS]` token is present but the `[SEP]` token is missing from the encoding (it is replaced with a `[PAD]` token). Thus, even if the text is truncated, there is thus at least one `[PAD]` token between the text and the relations. The value `0` in the `attention_mask` or `input_ids` incidates `[PAD]` tokens. Overall, we can write the structure as\n",
    "\n",
    "```\n",
    "[CLS] (text) [PAD]+ (relations)\n",
    "```\n",
    "\n",
    "`token_type_ids` is `1` for relation tokens and `0` for everything else. Thus, to get the number of text tokens (or equivalently, the index of the first relation token), we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd14d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"token_type_ids\"].argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4225a91",
   "metadata": {},
   "source": [
    "Notably, this is the same as `token_len_batch` so we can conclude that field stores the length of the text in the embedding. This is also the same as `text_len + 2`. That is, the encoding can contain up to `text_len` non-special text tokens, plus a leading `[CLS]` and a trailing `[PAD]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0867a7",
   "metadata": {},
   "source": [
    "### Matrix Fields\n",
    "\n",
    "The other fields are `head_label`, `span_label`, and `tail_label` corresponding to the three ground truth interaction matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b8232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length, = example[\"input_ids\"].shape\n",
    "assert example[\"head_label\"].shape \\\n",
    "    == example[\"span_label\"].shape \\\n",
    "    == example[\"tail_label\"].shape \\\n",
    "    == (sequence_length, sequence_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniRel",
   "language": "python",
   "name": "unirel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
